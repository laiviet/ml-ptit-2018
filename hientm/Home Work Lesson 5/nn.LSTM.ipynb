{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mishuhashi/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/utils/fixes.py:64: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if 'order' in inspect.getargspec(np.copy)[0]:\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import write_data\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import models\n",
    "import model\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dict(filepath):\n",
    "        with open(filepath, 'r') as f:\n",
    "            temp = f.read().split('\\n')\n",
    "        result = {}\n",
    "        for line in temp:\n",
    "            if line:\n",
    "                temp2 = line.split()\n",
    "                result[temp2[0]] = np.array(list(float(i) for i in temp2[1:]))\n",
    "        return result\n",
    "\n",
    "vocab_dict = read_dict('glove.6B/glove.6B.50d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_index = {}\n",
    "embed = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "for key in vocab_dict:\n",
    "    vocab_index[key] = index\n",
    "    embed.append(vocab_dict[key])\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_index[r'\\unknow'] = index\n",
    "embed.append(np.array([random.random() for c in range(len(vocab_dict['.']))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = np.array(embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Compression(nn.Module):\n",
    "    def __init__(self, embedding, hidden_dim, output_dim):\n",
    "        super(Compression, self).__init__()\n",
    "        self.embedding = nn.Embedding(embedding.shape[0], embedding.shape[1])\n",
    "        self.embedding.weight.data = torch.tensor(embedding)\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        self.lstm = models.LSTM(embedding.shape[1], hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        temp = self.embedding(x)\n",
    "        y = self.lstm(temp.float())\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class data_loader(data.Dataset):\n",
    "    def read_file(self, file_path):\n",
    "        with open(file_path, 'r') as f:\n",
    "            temp = f.read().split('\\n')\n",
    "        if not temp[-1]:\n",
    "            temp.pop()\n",
    "        return temp\n",
    "    def word2vect(self, list_sentences):\n",
    "        result = []\n",
    "        for sentence in list_sentences:\n",
    "            temp = sentence.split(' ')\n",
    "            if not temp[-1]:\n",
    "                temp.pop()\n",
    "            count = 0\n",
    "            for i in temp:\n",
    "                if i not in vocab_dict:\n",
    "                    temp[count] = r'\\unknow'\n",
    "                count += 1\n",
    "            index = list(vocab_index[word] for word in temp)\n",
    "            result.append(np.array(index))\n",
    "        return result\n",
    "    def __init__(self, type = 0, transform=None, target_transform=None):\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.type = type    \n",
    "        if self.type == 0:\n",
    "            self.train_data = self.word2vect(self.read_file('train.ori'))\n",
    "            temp1 = self.read_file('train.bin')\n",
    "            self.train_labels = []\n",
    "            for label in temp1:\n",
    "                temp3 = list(label)\n",
    "                self.train_labels.append([int(i) for i in temp3])\n",
    "            count = 0\n",
    "            for i in self.train_data:\n",
    "                t1 = i.shape[0]\n",
    "                t2 = len(self.train_labels[count])\n",
    "                t3 = self.train_labels[count]\n",
    "                if t1 > t2:\n",
    "                    i = t2 - 1\n",
    "                    t3.pop()\n",
    "                    while i < t1 - 1:\n",
    "                        t3.append(0)\n",
    "                        i += 1\n",
    "                    t3.append(1)\n",
    "                    self.train_labels[count] = t3\n",
    "                if t1 < t2:\n",
    "                    i = t2\n",
    "                    t3.pop()\n",
    "                    while i > t1 - 1:\n",
    "                        t3.pop()\n",
    "                        i -= 1\n",
    "                    t3.append(1)\n",
    "                    self.train_labels[count] = t3\n",
    "                count += 1\n",
    "        else:\n",
    "            if self.type == 1:\n",
    "                self.test_data = self.word2vect(self.read_file('test.ori'))\n",
    "                temp1 = self.read_file('test.bin')\n",
    "                self.test_labels = []\n",
    "                for label in temp1:\n",
    "                    temp3 = list(label)\n",
    "                    self.test_labels.append([int(i) for i in temp3])\n",
    "                count = 0\n",
    "                for i in self.test_data:\n",
    "                    t1 = i.shape[0]\n",
    "                    t2 = len(self.test_labels[count])\n",
    "                    t3 = self.test_labels[count]\n",
    "                    if t1 > t2:\n",
    "                        i = t2 - 1\n",
    "                        t3.pop()\n",
    "                        while i < t1 - 1:\n",
    "                            t3.append(0)\n",
    "                            i += 1\n",
    "                        t3.append(1)\n",
    "                        self.test_labels[count] = t3\n",
    "                    if t1 < t2:\n",
    "                        i = t2 - 1\n",
    "                        t3.pop()\n",
    "                        while i > t1 - 1:\n",
    "                            t3.pop()\n",
    "                            i -= 1\n",
    "                        t3.append(1)\n",
    "                        self.test_labels[count] = t3\n",
    "                    count += 1\n",
    "                \n",
    "            else:\n",
    "                self.valid_data = self.word2vect(self.read_file('valid.ori'))\n",
    "                temp1 = self.read_file('valid.bin')\n",
    "                self.valid_labels = []\n",
    "                for label in temp1:\n",
    "                    temp3 = list(label)\n",
    "                    self.valid_labels.append([int(i) for i in temp3])\n",
    "                count = 0\n",
    "                for i in self.valid_data:\n",
    "                    t1 = i.shape[0]\n",
    "                    t2 = len(self.valid_labels[count])\n",
    "                    t3 = self.valid_labels[count]\n",
    "                    if t1 > t2:\n",
    "                        i = t2 - 1\n",
    "                        t3.pop()\n",
    "                        while i < t1 - 1:\n",
    "                            t3.append(0)\n",
    "                            i += 1\n",
    "                        t3.append(1)\n",
    "                        self.valid_labels[count] = t3\n",
    "                    if t1 < t2:\n",
    "                        i = t2 - 1\n",
    "                        t3.pop()\n",
    "                        while i > t1 - 1:\n",
    "                            t3.pop()\n",
    "                            i -= 1\n",
    "                        t3.append(1)\n",
    "                        self.valid_labels[count] = t3\n",
    "                    count += 1\n",
    "                \n",
    "    def __getitem__(self, index):\n",
    "        if self.type == 0:\n",
    "            img, target = self.train_data[index], self.train_labels[index]\n",
    "        else:\n",
    "            if self.type == 1:\n",
    "                img, target = self.test_data[index], self.test_labels[index]\n",
    "            else:\n",
    "                img, target = self.valid_data[index], self.valid_labels[index]\n",
    "        return img, target\n",
    "    def __len__(self):\n",
    "        if self.type == 0:\n",
    "            return len(self.train_data)\n",
    "        else:\n",
    "            if self.type == 1:\n",
    "                return len(self.test_data)\n",
    "            else:\n",
    "                return len(self.valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = data_loader(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = data_loader(1)\n",
    "valid_set = data_loader(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = data.DataLoader(train_set, batch_size=1, shuffle=False, num_workers=0)\n",
    "testloader = data.DataLoader(test_set, batch_size=1, shuffle=False, num_workers=0)\n",
    "validloader = data.DataLoader(valid_set, batch_size=1, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Compression(embed,30, 2)\n",
    "epoch = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 7858/7858 [01:39<00:00, 78.75it/s]\n",
      "Testing: 100%|██████████| 981/981 [00:07<00:00, 138.14it/s]\n",
      "Valid: 100%|██████████| 978/978 [00:06<00:00, 155.20it/s]\n",
      "Training:   0%|          | 9/7858 [00:00<01:35, 82.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train loss: 6217.809907749295\n",
      "Test loss: 635.7367730736732 | Test accuracy: 0.5952827974653837\n",
      "Valid loss: 631.1216077804565 | Valid accuracy: 0.6022762706579358\n",
      "------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 7858/7858 [01:34<00:00, 83.12it/s]\n",
      "Testing: 100%|██████████| 981/981 [00:05<00:00, 171.09it/s]\n",
      "Valid: 100%|██████████| 978/978 [00:05<00:00, 169.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2\n",
      "Train loss: 5864.409089520574\n",
      "Test loss: 602.0436209142208 | Test accuracy: 0.6025971994054604\n",
      "Valid loss: 600.6418030560017 | Valid accuracy: 0.6138525101340817\n",
      "------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for e in range(epoch):\n",
    "    # Train\n",
    "    train_loss = 0.0\n",
    "    for feature, label in tqdm(trainloader, desc=\"Training\"):\n",
    "        feature, label = Variable(torch.LongTensor(feature)), Variable(torch.tensor(label).squeeze())\n",
    "        label = label.reshape(trainloader.batch_size, -1)\n",
    "        y_hat = model(feature)\n",
    "        loss = criterion(y_hat, label)\n",
    "        train_loss += loss.data.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    # Test\n",
    "    test_loss = 0.0\n",
    "    test_acc = 0.0\n",
    "    y = []\n",
    "    outputs = []\n",
    "    count = 0\n",
    "    for feature, label in tqdm(testloader, desc=\"Testing\"):\n",
    "        temp = label\n",
    "        #y += label\n",
    "        feature, label = Variable(torch.LongTensor(feature)), Variable(torch.tensor(label).squeeze())\n",
    "        label = label.reshape(testloader.batch_size, -1)\n",
    "        y_hat = model(feature)\n",
    "        loss = criterion(y_hat, label)\n",
    "        train_loss += loss.data.item()\n",
    "        test_loss += loss.data.item()\n",
    "        y_hat = y_hat.reshape(y_hat.shape[0], y_hat.shape[2], y_hat.shape[1])\n",
    "        y_hat = np.argmax(y_hat.data, axis=2).view(testloader.batch_size, -1)\n",
    "        count1 = 0\n",
    "        total = 0\n",
    "        for i in label:\n",
    "            count2 = 0\n",
    "            for j in i:\n",
    "                if y_hat[count1][count2] == j:\n",
    "                    test_acc += 1\n",
    "                count2 += 1\n",
    "                total += 1\n",
    "            count1 += 1\n",
    "        count += total\n",
    "        #outputs += y_hat.tolist()\n",
    "    test_acc = test_acc / count\n",
    "    #test_acc = accuracy_score(y, outputs)\n",
    "    \n",
    "    # Valid\n",
    "    valid_loss = 0.0\n",
    "    valid_acc = 0.0\n",
    "    y = []\n",
    "    outputs = []\n",
    "    count = 0\n",
    "    for feature, label in tqdm(validloader, desc=\"Valid\"):\n",
    "        temp = label\n",
    "        #y += label\n",
    "        feature, label = Variable(torch.LongTensor(feature)), Variable(torch.tensor(label).squeeze())\n",
    "        label = label.reshape(validloader.batch_size, -1)\n",
    "        y_hat = model(feature)\n",
    "        loss = criterion(y_hat, label)\n",
    "        train_loss += loss.data.item()\n",
    "        valid_loss += loss.data.item()\n",
    "        y_hat = y_hat.reshape(y_hat.shape[0], y_hat.shape[2], y_hat.shape[1])\n",
    "        y_hat = np.argmax(y_hat.data, axis=2).view(testloader.batch_size, -1)\n",
    "        count1 = 0\n",
    "        total = 0\n",
    "        for i in label:\n",
    "            count2 = 0\n",
    "            for j in i:\n",
    "                if y_hat[count1][count2] == j:\n",
    "                    valid_acc += 1\n",
    "                count2 += 1\n",
    "                total += 1\n",
    "            count1 += 1\n",
    "        count += total\n",
    "        #outputs += y_hat.tolist()\n",
    "    valid_acc = valid_acc / count\n",
    "    print(\"Epoch: {}\".format(e + 1))\n",
    "    print('Train loss: {}'.format(train_loss))\n",
    "    print('Test loss: {} | Test accuracy: {}'.format(test_loss, test_acc))\n",
    "    print('Valid loss: {} | Valid accuracy: {}'.format(valid_loss, valid_acc))\n",
    "    print('------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
