{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import torch.utils.data as data\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cifar10(data.Dataset):\n",
    "    root = 'data/cifar-10-batches-py/{}'\n",
    "    train_list = ['data_batch_1',\n",
    "                  'data_batch_2',\n",
    "                  'data_batch_3',\n",
    "                  'data_batch_4']\n",
    "    valid_list = ['data_batch_5']\n",
    "    test_list = ['test_batch']\n",
    "    def __init__(self, type = 0, transform=None, target_transform=None):\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.type = type\n",
    "        \n",
    "        if self.type == 0:\n",
    "            self.train_data = []\n",
    "            self.train_labels = []\n",
    "            for f in self.train_list:\n",
    "                file = self.root.format(f)\n",
    "                with open(file, 'rb') as fo:\n",
    "                    entry = pickle.load(fo, encoding='latin1')\n",
    "                    self.train_data.append(entry['data'])\n",
    "                    self.train_labels += entry['labels']\n",
    "            self.train_data = np.concatenate(self.train_data)\n",
    "            self.train_data = self.train_data.reshape(40000, 3, 32, 32)\n",
    "        else:\n",
    "            if self.type == 1:\n",
    "                f = self.test_list[0]\n",
    "                file = self.root.format(f)\n",
    "                with open(file, 'rb') as fo:\n",
    "                    entry = pickle.load(fo, encoding='latin1')\n",
    "                    self.test_data = entry['data']\n",
    "                    self.test_labels = entry['labels']\n",
    "                self.test_data = self.test_data.reshape(10000, 3, 32, 32)\n",
    "            else:\n",
    "                f = self.valid_list[0]\n",
    "                file = self.root.format(f)\n",
    "                with open(file, 'rb') as fo:\n",
    "                    entry = pickle.load(fo, encoding='latin1')\n",
    "                    self.valid_data = entry['data']\n",
    "                    self.valid_labels = entry['labels']\n",
    "                self.valid_data = self.valid_data.reshape(10000, 3, 32, 32)\n",
    "    def __getitem__(self, index):\n",
    "        if self.type == 0:\n",
    "            img, target = self.train_data[index], self.train_labels[index]\n",
    "        else:\n",
    "            if self.type == 1:\n",
    "                img, target = self.test_data[index], self.test_labels[index]\n",
    "            else:\n",
    "                img, target = self.valid_data[index], self.valid_labels[index]\n",
    "        return img, target\n",
    "    def __len__(self):\n",
    "        if self.type == 0:\n",
    "            return len(self.train_data)\n",
    "        else:\n",
    "            if self.type == 1:\n",
    "                return len(self.test_data)\n",
    "            else:\n",
    "                return len(self.valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = Cifar10(0)\n",
    "test_set = Cifar10(1)\n",
    "valid_set = Cifar10(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = data.DataLoader(train_set, batch_size=128, shuffle=True, num_workers=4)\n",
    "testloader = data.DataLoader(test_set, batch_size=128, shuffle=False, num_workers=4)\n",
    "validloader = data.DataLoader(valid_set, batch_size=128, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "num_of_classes = len(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.ConvLayers = nn.Sequential(\n",
    "            # Input layer 32x32x3\n",
    "            nn.Conv2d(3, 96, kernel_size=3, padding=1, stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # Convolutional layer 1: (32 - 2 + 2)/2 16x16x96\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            # 8x8x96\n",
    "            nn.Conv2d(96, 256, kernel_size=3, padding=1, groups=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # Convolutional layer 2: 8x8x256\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            # 4x4x256 \n",
    "            nn.Conv2d(256, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # Convolutional layer 3: 4x4x384\n",
    "            nn.Conv2d(384, 384, kernel_size=3, padding=1, groups=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # Convolutional layer 4: 4x4x384\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1, groups=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "            # Convolutional layer 4: 2x2x256\n",
    "        )\n",
    "        self.FullyConnectedLayers = nn.Sequential(\n",
    "            # Reducing overfitting\n",
    "            nn.Dropout(),\n",
    "            # Fully Connected Layer 1\n",
    "            nn.Linear(256*2*2, 4096),\n",
    "            # Fully Connected Layer 2\n",
    "            nn.Linear(4096, 4096),\n",
    "            # Output Layer\n",
    "            nn.Linear(4096, num_of_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.ConvLayers(x)\n",
    "        x = x.view(x.size(0), 256*2*2)\n",
    "        x = self.FullyConnectedLayers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AlexNet()\n",
    "lr = 0.001\n",
    "epoch = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AlexNet(\n",
       "  (ConvLayers): Sequential(\n",
       "    (0): Conv2d(3, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (1): ReLU(inplace)\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(96, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): ReLU(inplace)\n",
       "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(256, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU(inplace)\n",
       "    (8): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU(inplace)\n",
       "    (10): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace)\n",
       "    (12): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (FullyConnectedLayers): Sequential(\n",
       "    (0): Dropout(p=0.5)\n",
       "    (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "    (2): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (3): Linear(in_features=4096, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 313/313 [00:04<00:00, 75.31it/s]\n",
      "Testing: 100%|██████████| 79/79 [00:00<00:00, 133.66it/s]\n",
      "Valid: 100%|██████████| 79/79 [00:00<00:00, 136.28it/s]\n",
      "Training:   0%|          | 0/313 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train loss: 568.4030133485794\n",
      "Test loss: 123.1168247461319 | Test accuracy: 0.4339\n",
      "Valid loss: 125.44561207294464 | Valid accuracy: 0.423\n",
      "------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 313/313 [00:04<00:00, 74.55it/s]\n",
      "Testing: 100%|██████████| 79/79 [00:00<00:00, 131.77it/s]\n",
      "Valid: 100%|██████████| 79/79 [00:00<00:00, 135.57it/s]\n",
      "Training:   0%|          | 0/313 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2\n",
      "Train loss: 453.03020453453064\n",
      "Test loss: 106.68509197235107 | Test accuracy: 0.5099\n",
      "Valid loss: 108.27784025669098 | Valid accuracy: 0.5087\n",
      "------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 313/313 [00:04<00:00, 74.37it/s]\n",
      "Testing: 100%|██████████| 79/79 [00:00<00:00, 137.75it/s]\n",
      "Valid: 100%|██████████| 79/79 [00:00<00:00, 133.65it/s]\n",
      "Training:   0%|          | 0/313 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3\n",
      "Train loss: 401.0893175601959\n",
      "Test loss: 94.8061021566391 | Test accuracy: 0.5693\n",
      "Valid loss: 96.76968145370483 | Valid accuracy: 0.5627\n",
      "------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 313/313 [00:04<00:00, 74.36it/s]\n",
      "Testing: 100%|██████████| 79/79 [00:00<00:00, 132.86it/s]\n",
      "Valid: 100%|██████████| 79/79 [00:00<00:00, 137.73it/s]\n",
      "Training:   0%|          | 0/313 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4\n",
      "Train loss: 363.919276535511\n",
      "Test loss: 90.11675482988358 | Test accuracy: 0.5891\n",
      "Valid loss: 90.58812892436981 | Valid accuracy: 0.5916\n",
      "------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 313/313 [00:04<00:00, 74.04it/s]\n",
      "Testing: 100%|██████████| 79/79 [00:00<00:00, 125.38it/s]\n",
      "Valid: 100%|██████████| 79/79 [00:00<00:00, 128.89it/s]\n",
      "Training:   0%|          | 0/313 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5\n",
      "Train loss: 332.821211874485\n",
      "Test loss: 85.53417098522186 | Test accuracy: 0.6109\n",
      "Valid loss: 86.11614775657654 | Valid accuracy: 0.6131\n",
      "------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 313/313 [00:04<00:00, 73.92it/s]\n",
      "Testing: 100%|██████████| 79/79 [00:00<00:00, 132.22it/s]\n",
      "Valid: 100%|██████████| 79/79 [00:00<00:00, 133.87it/s]\n",
      "Training:   0%|          | 0/313 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6\n",
      "Train loss: 307.07700884342194\n",
      "Test loss: 83.27458280324936 | Test accuracy: 0.6219\n",
      "Valid loss: 83.52156734466553 | Valid accuracy: 0.6289\n",
      "------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 313/313 [00:04<00:00, 73.30it/s]\n",
      "Testing: 100%|██████████| 79/79 [00:00<00:00, 134.22it/s]\n",
      "Valid: 100%|██████████| 79/79 [00:00<00:00, 136.17it/s]\n",
      "Training:   0%|          | 0/313 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7\n",
      "Train loss: 281.13493329286575\n",
      "Test loss: 77.79295885562897 | Test accuracy: 0.6483\n",
      "Valid loss: 77.1319227218628 | Valid accuracy: 0.6614\n",
      "------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 313/313 [00:04<00:00, 73.47it/s]\n",
      "Testing: 100%|██████████| 79/79 [00:00<00:00, 131.59it/s]\n",
      "Valid: 100%|██████████| 79/79 [00:00<00:00, 130.84it/s]\n",
      "Training:   0%|          | 0/313 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8\n",
      "Train loss: 261.8619310259819\n",
      "Test loss: 75.61498057842255 | Test accuracy: 0.6695\n",
      "Valid loss: 75.16856843233109 | Valid accuracy: 0.6681\n",
      "------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 313/313 [00:04<00:00, 73.31it/s]\n",
      "Testing: 100%|██████████| 79/79 [00:00<00:00, 120.53it/s]\n",
      "Valid: 100%|██████████| 79/79 [00:00<00:00, 148.87it/s]\n",
      "Training:   0%|          | 0/313 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9\n",
      "Train loss: 239.9566462635994\n",
      "Test loss: 75.60657519102097 | Test accuracy: 0.6655\n",
      "Valid loss: 74.56170147657394 | Valid accuracy: 0.6784\n",
      "------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 313/313 [00:04<00:00, 73.01it/s]\n",
      "Testing: 100%|██████████| 79/79 [00:00<00:00, 113.65it/s]\n",
      "Valid: 100%|██████████| 79/79 [00:00<00:00, 137.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10\n",
      "Train loss: 220.55192869901657\n",
      "Test loss: 74.62104737758636 | Test accuracy: 0.6805\n",
      "Valid loss: 73.41278940439224 | Valid accuracy: 0.6851\n",
      "------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for e in range(epoch):\n",
    "    # Train\n",
    "    train_loss = 0.0\n",
    "    for feature, label in tqdm(trainloader, desc=\"Training\"):\n",
    "        feature, label = Variable(feature.float()).to(device), Variable(label.squeeze()).to(device)\n",
    "        y_hat = model(feature)\n",
    "        loss = criterion(y_hat, label)\n",
    "        train_loss += loss.data.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    # Test\n",
    "    test_loss = 0.0\n",
    "    y = []\n",
    "    outputs = []\n",
    "    for feature, label in tqdm(testloader, desc=\"Testing\"):\n",
    "        y += label.numpy().tolist()\n",
    "        feature, label = Variable(feature.float()).to(device), Variable(label.squeeze()).to(device)\n",
    "        y_hat = model(feature)\n",
    "        loss = criterion(y_hat, label)\n",
    "        test_loss += loss.data.item()\n",
    "        y_hat = np.argmax(y_hat.data, axis=1)\n",
    "        outputs += y_hat.tolist()\n",
    "    test_acc = accuracy_score(y, outputs)\n",
    "    \n",
    "    # Valid\n",
    "    valid_loss = 0.0\n",
    "    y = []\n",
    "    outputs = []\n",
    "    for feature, label in tqdm(validloader, desc=\"Valid\"):\n",
    "        y += label.numpy().tolist()\n",
    "        feature, label = Variable(feature.float()).to(device), Variable(label.squeeze()).to(device)\n",
    "        y_hat = model(feature)\n",
    "        loss = criterion(y_hat, label)\n",
    "        valid_loss += loss.data.item()\n",
    "        y_hat = np.argmax(y_hat.data, axis=1)\n",
    "        outputs += y_hat.tolist()\n",
    "    valid_acc = accuracy_score(y, outputs)\n",
    "    print(\"Epoch: {}\".format(e + 1))\n",
    "    print('Train loss: {}'.format(train_loss))\n",
    "    print('Test loss: {} | Test accuracy: {}'.format(test_loss, test_acc))\n",
    "    print('Valid loss: {} | Valid accuracy: {}'.format(valid_loss, valid_acc))\n",
    "    print('------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/student/.conda/envs/hientm/lib/python3.6/site-packages/torch/serialization.py:193: UserWarning: Couldn't retrieve source code for container of type AlexNet. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "torch.save(model, 'alexnet.pt')\n",
    "temp = model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of plane : 75 %\n",
      "Accuracy of   car : 71 %\n",
      "Accuracy of  bird : 54 %\n",
      "Accuracy of   cat : 44 %\n",
      "Accuracy of  deer : 59 %\n",
      "Accuracy of   dog : 36 %\n",
      "Accuracy of  frog : 63 %\n",
      "Accuracy of horse : 60 %\n",
      "Accuracy of  ship : 78 %\n",
      "Accuracy of truck : 69 %\n"
     ]
    }
   ],
   "source": [
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        images, labels = Variable(images.float()), Variable(labels.squeeze())\n",
    "        outputs = temp(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        c = (predicted == labels).squeeze()\n",
    "        for i in range(4):\n",
    "            label = labels[i]\n",
    "            class_correct[label] += c[i].item()\n",
    "            class_total[label] += 1\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    print('Accuracy of %5s : %2d %%' % (\n",
    "        classes[i], 100 * class_correct[i] / class_total[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
